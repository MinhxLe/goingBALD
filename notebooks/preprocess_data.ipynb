{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/minh/Research/goingBALD\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Research/goingBALD\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bald.data import conll2003_utils as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"artifacts/data/raw/CoNLL2003/eng.train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PROCESSED_DIR = \"artifacts/data/processed/CoNLL2003/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_utils.load_raw_dataset(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate tags\n",
    "tags = data_utils.generate_NER_tag_set(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.save(os.path.join(DATA_PROCESSED_DIR, \"tags_idx.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'PER', 'ORG', 'LOC', 'MISC']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags.idx2key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating word2vec on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bald.parameters import WORD_EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing coNLL data to list of raw sentences, 1 per line\n",
    "# TODO do we need to lowercase the words and strip symbols\n",
    "RAW_SENTENCE_FNAME =  os.path.join(DATA_PROCESSED_DIR, \"eng.train.raw_sentences.txt\")\n",
    "raw_sentences = []\n",
    "word_set = set()\n",
    "with open(RAW_SENTENCE_FNAME, 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        orig_sentence = [word['word'] for word in sentence]\n",
    "        raw_sentences.append(orig_sentence)\n",
    "        f.write(\" \".join(raw_sentence) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using CBOW\n",
    "# negative is negative sampling (https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68)\n",
    "model = Word2Vec(raw_sentences, sg=1, size=WORD_EMBEDDING_SIZE, negative=8, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23623, 300)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.save_word2vec_format(\n",
    "    os.path.join(DATA_PROCESSED_DIR, \"word2vec.txt\"),\n",
    "    fvocab=os.path.join(DATA_PROCESSED_DIR, \"word2vec_vocab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(os.path.join(DATA_PROCESSED_DIR, \"word2vec.vector.npy\"), word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format(os.path.join(DATA_PROCESSED_DIR, \"word2vec.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23623, 300)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-92d3f34f90f8>:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  model.similarity(\"hardened\", \"throughput\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9985663"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"hardened\", \"throughput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the word indxer for word2vec\n",
    "from bald.data.indexer import Vocabulary\n",
    "word2vec_vocab = Vocabulary()\n",
    "for word in word_vectors.index2word:\n",
    "    word2vec_vocab.add(word)\n",
    "word2vec_vocab.save(os.path.join(DATA_PROCESSED_DIR, \"word2vec_vocab_idx.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', '.', ',', 'the', 'of', 'in', 'to', 'a', '(']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_vocab.idx2key[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bald.data.indexer import Vocabulary, Indexer\n",
    "tag_set = Indexer()\n",
    "word2vec_vocab = Vocabulary()\n",
    "tag_set.load(os.path.join(DATA_PROCESSED_DIR, \"tags_idx.txt\"))\n",
    "word2vec_vocab.load(os.path.join(DATA_PROCESSED_DIR, \"word2vec_vocab_idx.txt\"))\n",
    "\n",
    "raw_sentences = data_utils.load_raw_dataset(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "sentences = []\n",
    "tags = []\n",
    "for sentence in raw_sentences:\n",
    "    sentence_idx = []\n",
    "    tag_idx = []\n",
    "    for word_data in sentence:\n",
    "        sentence_idx.append(word2vec_vocab[word_data['word']])\n",
    "        tag_idx.append(tag_set[word_data['NER_tag']])\n",
    "    sentences.append(sentence_idx)\n",
    "    tags.append(tag_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(sentences, open(os.path.join(DATA_PROCESSED_DIR, \"sentences.train.pkl\"), 'wb'))\n",
    "pickle.dump(tags, open(os.path.join(DATA_PROCESSED_DIR, \"NER_tags.train.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
